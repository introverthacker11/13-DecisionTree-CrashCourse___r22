{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfbd9816",
   "metadata": {},
   "source": [
    "###\n",
    "## Decision Tree Parameters\n",
    "train and test accuracy\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c498690e",
   "metadata": {},
   "source": [
    "## 1. criterion\n",
    "\n",
    "Function to measure the quality of a split.\n",
    "\n",
    "Options:\n",
    "\n",
    "- \"gini\" (Gini Impurity)\n",
    "- \"entropy\" (Information Gain)\n",
    "- \"log_loss\" (used for classification).\n",
    "\n",
    "## 1.1 Gini Impurity\n",
    "\n",
    "### What is it?\n",
    "Gini Impurity is a measure of how \"impure\" or \"mixed\" a group of data is. It tells us the chance that a randomly picked item from the group would be labeled incorrectly if we guessed the label randomly.\n",
    "\n",
    "- A **Gini Impurity of 0** means the group is completely pure (all items belong to the same class).  \n",
    "- A higher Gini value (closer to 1) means the group is more mixed.\n",
    "\n",
    "---\n",
    "\n",
    "### How is it calculated?\n",
    "The formula for Gini Impurity is:\n",
    "\n",
    "Gini=1−∑(p square i)\n",
    "\n",
    "Where:\n",
    "- (pi) = proportion of items in the group belonging to class i.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "Imagine we have a basket with 10 fruits:\n",
    "- 7 Apples\n",
    "- 3 Oranges\n",
    "\n",
    "#### Step 1: Calculate the proportions\n",
    "- Proportion of apples : 7/10 = 0.7\n",
    "- Proportion of oranges : 3/10 = 0.3\n",
    "\n",
    "#### Step 2: Plug into the formula\n",
    "\n",
    "Gini = 1 - (p square apples + p square oranges)\n",
    "\n",
    "Gini = 1 - (0.7 * 2 + 0.3 * 2)\n",
    "\n",
    "Gini = 1 - (0.49 + 0.09) = 1 - 0.58 = 0.42\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "- A **Gini Impurity of 0.42** means the group is somewhat mixed but not completely random.\n",
    "- If the basket had only apples (10 apples, 0 oranges), the Gini would be:\n",
    "\n",
    "\n",
    "Gini = 1 - (1 * 2 + 0 * 2) = 1 - 1 = 0\n",
    "\n",
    "This means the group is pure.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Points\n",
    "- Gini Impurity helps decide the best split when building a decision tree.\n",
    "- **Lower Gini values** indicate purer groups, which are preferred.\n",
    "\n",
    "##\n",
    "\n",
    "## 1.2 Entropy\n",
    "\n",
    "### What is it?\n",
    "Entropy measures the level of **disorder** or **uncertainty** in a group of data. It’s used in decision trees to evaluate the quality of splits.  \n",
    "- A **low entropy** value (close to 0) means the group is **pure**, where all items belong to the same class (e.g., all \"Yes\" or all \"No\").\n",
    "- A **high entropy** value (closer to 1) means the group is **mixed**, containing a mix of different classes.\n",
    "\n",
    "The goal in decision trees is to create splits that lower the entropy, making groups purer.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula\n",
    "The formula for entropy is:\n",
    "\n",
    "Entropy = −∑(pi.log2(pi))\n",
    "\n",
    "Where:\n",
    "- pi is the proportion of items in the group belonging to class i.\n",
    "\n",
    "If pi = 0, we define pi(log2(p_i)) = 0 to avoid undefined values.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "Imagine a basket of 10 fruits:\n",
    "- 7 Apples\n",
    "- 3 Oranges\n",
    "\n",
    "#### Step 1: Calculate the proportions\n",
    "- Proportion of apples (papples) = 7/10 = 0.7\n",
    "- Proportion of oranges (papples) = 0.3/10= 0.3\n",
    "\n",
    "#### Step 2: Plug into the formula\n",
    "\n",
    "Entropy = -(papples.log2(papples) + poranges.log2(poranges))\n",
    "\n",
    "Substitute the values:\n",
    "-(0.7.log2(0.7) + 0.3.log2(0.3))\n",
    "\n",
    "#### Step 3: Calculate the log2 values\n",
    "Using approximate values:\n",
    "- log2(0.7) = -0.514\n",
    "- log2(0.3) = -1.737\n",
    "\n",
    "\n",
    "Now calculate:\n",
    "Entropy = −(0.7⋅−0.514 + 0.3⋅−1.737)\n",
    "Entropy= −(−0.36−0.52) = 0.88\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "- The **entropy of 0.88** means the group is **somewhat mixed** but not completely random.\n",
    "- If the basket had **only apples** (10 apples, 0 oranges), the entropy would be:\n",
    "  - (1.log2(1) + 0.log2(0))\n",
    "  - Entropy= −(1⋅0+0) = 0\n",
    "\n",
    "This means the group is **completely pure**.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Points\n",
    "- **Entropy** measures **disorder** or **uncertainty** in the group.\n",
    "- **Lower entropy** values indicate **purer** groups.\n",
    "- Decision trees aim to split data into groups with **lower entropy** to make the classifications more certain.\n",
    "\n",
    "##\n",
    "\n",
    "## 1.3 logloss (Logarithmic Loss)\n",
    "###\n",
    "**What is it?**\n",
    "Log Loss, also known as Logarithmic Loss, is a measure used to evaluate the performance of classification models, particularly in binary classification tasks (e.g., predicting \"Yes\" or \"No\"). It calculates the difference between the predicted probabilities and the actual labels, providing a way to measure the accuracy of probabilistic predictions.\n",
    "\n",
    "- Low Log Loss values indicate that the model's predictions are close to the true labels (high accuracy).\n",
    "- High Log Loss values indicate that the model is making poor predictions, with a greater difference between predicted probabilities and actual labels.\n",
    "\n",
    "Formula\n",
    "The formula for Log Loss in binary classification is:\n",
    "\n",
    "Log Loss = -(1/N) Σ [yᵢ * log(pᵢ) + (1 - yᵢ) * log(1 - pᵢ)]\n",
    "\n",
    "Where,\n",
    "- N = the number of data points (samples).\n",
    "- yi = the actual label of i-th sample(either 0 or 1)\n",
    "- pi = the predicted probability of the positive class (the probability yi = 1)\n",
    "\n",
    "##\n",
    "## 2 Max_Depth\n",
    "###\n",
    "Max Depth refers to the maximum number of levels or layers a decision tree can have. It determines how deep the tree can grow, where each level represents a decision based on the data features.\n",
    "\n",
    "- Shallow trees (low max depth) tend to underfit the data, as they don't capture enough complexity.\n",
    "- Deep trees (high max depth) can overfit the data, meaning they might perform well on training data but poorly on unseen data because they capture noise and outliers.\n",
    "\n",
    "**Why is it important?**\n",
    "\n",
    "Max Depth is a crucial hyperparameter in decision tree models. By controlling the tree's depth, you balance underfitting and overfitting:\n",
    "\n",
    "- Underfitting happens when the tree is too shallow and doesn't capture enough patterns in the data.\n",
    "- Overfitting happens when the tree is too deep and fits noise or small fluctuations in the training data.\n",
    "\n",
    "**Scenario 1: Max Depth = 1 (Shallow Tree)**\n",
    "\n",
    "If we set max_depth = 1, the tree can only make one decision. It may split on the study hours feature and decide a threshold (say, 4 hours of study) to classify the students.\n",
    "\n",
    "The decision tree might look like this:\n",
    "\n",
    "Study Hours <= 4?  \n",
    "    |  \n",
    "   Yes -> Fail (0)  \n",
    "   No -> Pass (1)\n",
    "\n",
    "- This tree doesn't capture the full complexity of the dataset. It classifies students with fewer than 4 study hours as failing and those with more than 4 study hours as passing.\n",
    "\n",
    "- Underfitting: The model is too simple, and it doesn't distinguish between the students who studied 2 hours and 4 hours, for example.\n",
    "\n",
    "**Scenario 3: Max Depth = 3 (Very Deep Tree)**\n",
    "\n",
    "If we set max_depth = 3, the tree can grow even deeper and may end up overfitting the data:\n",
    "\n",
    "Study Hours <= 4?  \n",
    "    |  \n",
    "   Yes -> Fail (0)  \n",
    "   No -> Study Hours <= 6?  \n",
    "          |  \n",
    "        Yes -> Pass (1)  \n",
    "        No -> Study Hours <= 8?  \n",
    "               |  \n",
    "             Yes -> Pass (1)  \n",
    "             No -> Pass (1)\n",
    "\n",
    "- The tree is perfectly fitting the training data. It could classify each student based on their exact study hours.\n",
    "- Overfitting: The model is too complex and will likely perform poorly on new, unseen data because it's too specific to the training data.\n",
    "\n",
    "**When to use low value:**\n",
    "- To prevent overfitting on small or noisy datasets.\n",
    "- When interpretability is important (e.g., simple models for decision-making).\n",
    "- For shallow datasets with fewer features or less complexity.\n",
    "\n",
    "**When to use low value:**\n",
    "- For complex datasets with high feature interaction or large amounts of data.\n",
    "- When you prioritize high accuracy and are less concerned about overfitting.\n",
    "- In ensemble methods like Random Forests, where overfitting is controlled by averaging across trees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd38c06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c381bc5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853cf0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7973e5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
